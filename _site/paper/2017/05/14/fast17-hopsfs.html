<!DOCTYPE html>
<html lang="en-us">
  
  <head>
  <meta charset="UTF-8">
  <title>分享技术 品味生活</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>

  <body>
    <section class="page-header">
  <h1 class="project-name">分享技术 品味生活</h1>
  <h2 class="project-tagline">talk is cheap, show me the code</h2>
  <a href="https://github.com/qinglongyanyue/blog" class="btn">View on GitHub</a>
</section>

    <section class="main-content">
      
      <p>一直在关注文件系统领域，对于将文件系统元数据存在各类DB也是有点兴趣，关注到的论文包括CMU早期发表的TableFS，IndexFS，已经近期FAST15和16年的BetrFS，主要是基于key-value DB来加速元数据存储。</p>

<p>这篇论文则更进一步，考虑用NewSQL来存放元数据，不知道FS与NewSQL能擦出什么样的火花呢？来看看这篇文章吧。</p>

<h2 id="1-背景">1. 背景</h2>

<p>任何论文最重要的就是他的出发点，要解决啥问题，看看本文作者看到的痛点是啥？</p>

<p>随着NewSQL DB的快速发展，分布式DB的性能，扩展性，事务性能，新的内存介质，使得我们又得考虑将分布式FS的元数据架在NewSQL之上啦。</p>

<p>NewSQL多数都是以Google Spanner为蓝本，比如国内做的不错的TiDB，国外的cockroachDB都获得了极大的关注，以及学术界Yale大学的CalvinDB。其实Yale的Alexander Thomson教授在FAST2015就有篇CalvinFS就是基于CalvinDB设计的分布式FS，其中更关注跨地域的分布式FS。</p>

<p>这篇论文更多的关注的集群内部的元数据管理，并且以HDFS的元数据管理为例，阐明了作者的方案的价值。。</p>

<p>作者来自瑞典名校皇家工学院，合作者有oracle的专家，看样子oracle有资助这个事情，虽然并不是传统的存储名校，但是主题关系比较大，还是值得分析下。</p>

<p>作者认为当前有几个痛点：</p>

<blockquote>

  <ol>
    <li>当前的分布式FS基本都是为大文件设计的，能通过将数据切割个不同节点并发提升性能，但是对于文件系统元数据的处理就很挫了。</li>
    <li>仍有不少分布式FS将元数据放在单点，比如HDFS；或者将元数据分布式打散放在多个DISK中，比如GPFS，GFS等；或者直接简单将元数据sharding称不同的名字空间，每个名字空间的元数据独立管理。</li>
    <li>现有各种方案都不够完美，如果把元数据放到一个牛逼的分布式数据库搞定多好，支持事务，横向扩展，in-memory处理，SQL语义。但是FS的元数据是树形结构，如何能高效的映射到数据中呢？本文的核心就是搞定这个。</li>
  </ol>
</blockquote>

<p>总体的设计思路通过改造HDFS实现，名为HopsFS，整个方案的核心思路如下：</p>

<blockquote>

  <ol>
    <li>将HDFS的元数据存储和服务管理动作分离；使用NewSQL DB存放所有的元数据，NDB，后台是一个基于MySQL的分布式数据库；HopsFS在上层通过多个无状态的节点并发的去访问数据库，通过并发提升性能。</li>
    <li>HopsFS将文件系统的元数据操作包装为分布式事务，提交给NDB执行。并通过多种方式提升性能：
      <ul>
        <li>Batch多个操作，即每次事务尽量并发的发多个req下去</li>
        <li>开启write ahead cache，关于同一个事务的元数据先在内存聚合下再发下去</li>
        <li>通过NewSQL与FS的联合设计，比如FS将同一个子目录存放到NewSQL的同一个Partition或者sharding中，以及将事务尽量放在一个节点完成，以及提供高效的scan接口，方便ls类操作。</li>
        <li>提供一个inode hint cache，避免每次全路径的寻址开销。</li>
      </ul>
    </li>
    <li>对于少量的元数据操作，确实可以走数据的事务，但是低于mv和delete大目录这种同时需要巨多元数据的操作，直接使用数据的分布式事务显然是不现实的。
      <ul>
        <li>对于这类操作，论文引入了一种新的方法：使用应用级的分布式锁机制去隔离这种子树级的操作。在加锁之后，就可以使用数据库的事务去一小部分小部分的操作了。</li>
      </ul>
    </li>
    <li>HopsFS直接hack了HDFS，且在生产环境使用，且对于元数据写密集型负载，性能提升37倍。</li>
  </ol>
</blockquote>

<h2 id="2-hopsfs整体架构">2. HopsFS整体架构</h2>
<p><img src="/assets/hopsfs-1.png" alt="" /></p>

<p>整体架构非常简单：</p>
<ul>
  <li>原生的HDFS元数据部署方式:包括一个active的namenode，一个standby的namenode，至少3个journal node用来做日志存储（quorum算法），至少3个zk node用来做集群协同。搞了这么多事情，实际上上对外提供服务的只有active的namenode。。</li>
  <li>HopsFS的架构明显更加简单，无状态AA的多个namenode + 分布式数据库，性能好，逻辑简单。通过DB + nodeid选择一个当主（为啥不简单的用zk来选个主？），解决HDFS的心跳信息同步问题。</li>
  <li>每个HopsFS的namenode都有一个Data access layer（类似JDBC），封装所有针对分布式数据库的操作。</li>
  <li>客户端可以使用Round-Robin、random或者pin主的方案选择namenode；datanode汇报block的信息到leader namenode，leader再把block信息同步到所有其他namenode（这个信息量不大，可以简单这么玩）。</li>
</ul>

<h2 id="3-hopsfs分布式元数据">3. HopsFS分布式元数据</h2>
<p><img src="/assets/hopsfs-2.png" alt="" />
<img src="/assets/hopsfs-3.png" alt="" /></p>

<p>元数据设计的总体思路比较清晰：</p>
<ul>
  <li>一切从业务负载特征出发：从作者收集到的负载特征来看，或者业务公布的特征来看，list，file read和stat占据了95%的IO，如上面表格中的绿色填充。</li>
  <li>看看底层分布式数据的擅长啥，不擅长啥：跨shard的全表扫描，index scan都是DB消耗非常大的操作，所以要尽量少的去跨shard干活。而在一个shard内部，index scan，primary key的batch操作，primary key的单key操作都是非常高效的，所以要多用这块的能力。后面的思路全是围绕这条路来的。</li>
</ul>

<h3 id="31-文件系统到db的关系模型">3.1 文件系统到DB的关系模型</h3>
<p><img src="/assets/hopsfs-4.png" alt="" /></p>

<p>如上图所示，一切信息都映射到DB的表中：</p>

<p>实体类型：</p>
<ul>
  <li>inode实体：</li>
  <li>block实体：file的数据块</li>
  <li>副本实体：block的位置实体</li>
</ul>

<p>实体之间的关系：</p>
<ul>
  <li>目录inode直接对应inode表的一行，file inode除了自己的信息之后，还对应多个实体，包括块列表，每个块的位置以及checksum等，这些额外的实体需要额外的表来处理。</li>
  <li>目录和文件都是inode表中的一个实体，这个实体中还包括父目录的inode信息。</li>
  <li>inode表中不是存放全路径，而是父目录inode + file name</li>
  <li>每个file都会有1个或多个block，block的信息会在多个表中展示。（key应该是inode + block id？）
    <ul>
      <li>1）正常的block table；</li>
      <li>2）有节点失效，某些block的副本失效，此时block在URB表（低于正常副本数的表中）；</li>
      <li>3）URB表中的副本需要加入到PRB（pending replication table）表中进行恢复；</li>
      <li>4）当副本损坏时，数据块在CR表中；</li>
      <li>5)正在修复中的损坏block在RUC表中；</li>
      <li>6）有时候，副本数量超过设置量，加入到ER表；</li>
      <li>7)删除中的ER表的实体会放在Inv表中。</li>
    </ul>
  </li>
  <li>很重要的一点:inode实体中还包括一个外键，这个外键代表inode所在的partition，确保这个这个inode的所有操作都在一个DB的节点完成。</li>
</ul>

<h3 id="32-元数据分区方案">3.2 元数据分区方案</h3>

<p>先讨论没有热点的情况：</p>
<ul>
  <li>HopsFS直接使用父目录的inode id去分区元数据；同一个父目录下的所有inode元数据放在一个shard</li>
  <li>file inode相关的block信息，副本位置，checksum通过fileid来partition</li>
  <li>理想情况下（无热点），负载都在一个节点内部解决，基本没有跨节点的元数据操作，效率高</li>
</ul>

<h4 id="321-热点问题">3.2.1 热点问题</h4>

<p>如果有热点，怎么处理呢？</p>
<ul>
  <li>root肯定是最大的瓶颈，HopsFS直接在所有节点cache住root。且不支持修改root的任何元数据</li>
  <li>第一层目录也可能会是访问热点，默认可以配置第一层随机打散，后面的目录尽量聚合到一起</li>
  <li>当然也可以视负载特征把目录打散，但是会牺牲mv和ls的性能。</li>
</ul>

<h2 id="4-hopsfs事务操作">4. HopsFS事务操作</h2>
<p>HopsFS的事务操作包括2类：</p>
<ol>
  <li>inode类：针对一个实体（file，dir，block）操作的动作，比如mknod，mkdir，block stat改变等</li>
  <li>子树类：针对未知个inode操作的动作（可能达到上百万个），比如recursive del，mv，chmod，chown对一个目录。</li>
</ol>

<p>首先看看inode类操作如何映射到DB的操作，NDB（本文中的分布式mySql）最强的语义仅仅是read commit语义（<a href="http://www.cnblogs.com/ego/articles/1456317.html">关于DB语义的细节可以参考链接</a>）,但是FS的很多操作是需要串行语义的，如何解决呢：</p>
<ul>
  <li>使用行锁解决inode操作的并发冲突，但是一个事务中申请多个锁可能引起死锁，那怎么解决呢？
    <ol>
      <li>将所有的加锁动作顺序化，从根开始使用左序深度优先算法</li>
      <li>HDFS还有一些对inode读操作紧跟着写这个inode，这个数据库中会造成死锁（先拿读锁，升级到写锁），HopsFS直接将这些操作用提前用写锁。</li>
    </ol>
  </li>
</ul>

<h3 id="41-inode-hint-cache">4.1 Inode hint cache</h3>

<p>层层路径解析和check权限一直都是文件系统中最常见的元数据操作，如何加速这些操作呢？</p>

<p>论文使用hint机制加速路径解析：</p>
<ul>
  <li>每个name node都将将父目录inode + filename缓存起来（仅仅cache key），解析过程中可以并发的发起DB的查询操作。</li>
</ul>

<h4 id="411-cache一致性">4.1.1 cache一致性</h4>
<ul>
  <li>在文件系统操作之前，通过inode hint cache + DB的并发读取获取实际的inode信息，当cache失效时</li>
  <li>主key的读操作会失败，此时需要到DB中一层层解析，解析完成刷新内存</li>
  <li>cache一般不会经常失效，因为mv以及inode更新在Hadoop场景并不多</li>
</ul>

<h3 id="42-inode操作">4.2 inode操作</h3>
<p><img src="/assets/hopsfs-5.png" alt="" /></p>

<p>HopsFS从他的实际场景发现，大负载情况下，悲观并发设计的性能要高于乐观并发设计（即默认冲突比较多），所有的inode操作都三个独立的阶段，加锁，执行，update：</p>
<ul>
  <li>加锁阶段：
    <ol>
      <li>找到file对应的inode hint cache</li>
      <li>找到含有本操作相关所有信息，或者绝大部分信息的节点；事务就在这个节点执行</li>
      <li>并发去解析路径（直接用DB的Read commit语义， 不加锁）</li>
      <li>如果cache不命中，则层层去解析目录</li>
      <li>锁定并读取最后一个inode信息</li>
      <li>以及操作的类型，读取需要的数据</li>
      <li>HopsFS使用层级锁的概念，对于一个数的根，或者子树的根，意味着对整个树或者子树加锁；对inode加锁意味着整个inode的所有相关信息加锁</li>
    </ol>
  </li>
  <li>per-transactin cache：
    <ol>
      <li>事务过程中的所有读取到的信息都放在内存cache起来，避免多次访问DB</li>
      <li>数据的一致性通过行锁解决，除非事务结束，否则其他事务禁止修改cache的内容</li>
    </ol>
  </li>
  <li>执行和update：
    <ol>
      <li>每个inode的事务都在内存完成，执行完成之后，进入update阶段，以batch的方式批量提交到DB中</li>
    </ol>
  </li>
</ul>

<h2 id="5-处理大操作">5. 处理大操作</h2>
<p>针对目录的recursive的操作，可能会涉及百万级的inode，目前的DB还没有支持给百万条目加锁的，所以无法在一个事务中处理这么多的请求。这些操作包括move，delet，chown，chmod，set quota等。</p>

<h3 id="51-子树操作协议">5.1 子树操作协议</h3>
<p>由于DB无法支持那么大的事务，论文提出一种子树操作协议：</p>
<ul>
  <li>应用给子树加一个应用级分布式大锁，防止事务冲突
    <ol>
      <li>在子树操作结束之前，禁止任何新操作去访问这个子树</li>
      <li>在子树操作启动的之间，树是静止的（即树上没有修改操作在同时进行）</li>
      <li>出现错误时，不会出现孤儿inode或者不一致</li>
    </ol>
  </li>
</ul>

<p><img src="/assets/hopsfs-6.png" alt="" /></p>

<p>子树操作协议包括以下的步骤：</p>
<ol>
  <li>获取整个子树的root，加锁并持久化到DB，意味着这个子树全部加了写锁；加锁之前需要确认整个子树没有任何冲突的操作。有一个表存放所有加了锁的子树，方便查询锁冲突。</li>
  <li>为了让子树保持静止，子树操作需要在所有的子树操作结束之后再锁定整个子树;HopsFS通过一个线程池来并发的去DB中扫描inode状态，并读取inode到内存中，方便后续的操作。如果发现并发冲突，随机往后等一段时间在发起。</li>
  <li>将大目录操作切割为多个并发的事务去执行。（并发的事务如果有失败的怎么办？）加锁时从上往下，删除时从叶子往根，防止出现孤儿。</li>
</ol>

<h3 id="52-子树操作失败的处理">5.2 子树操作失败的处理</h3>

<p>如果在子树操作的过程中，节点down掉，怎么处理？HopsFS使用一种lazy cleanup的方案</p>
<ul>
  <li>每个namenode通过主namenode获取到active的namenode列表</li>
  <li>如果发现有一个subtree的锁属于一个dead namenode，那么这个锁直接回收掉。</li>
  <li>如果在删除操作过程中，如果节点失效，那么更换其他active的节点继续执行操作；如果是mv，chown之类的动作，主要对根目录操作（DB保证事务），此时只会存在忘记释放key的问题；通过第一步的方式搞定。</li>
</ul>

<h2 id="6-小结">6. 小结</h2>

<p>整体来看，论文的核心是将HDFS的元数据全部卸载到分布式数据库，并抽象出3种实体，很多table来解决元数据的各种问题。对于我们后续开发设计有较强的实用价值。</p>



      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">分享技术 品味生活</a> is maintained by <a href="l.ming@huawei.com">Author mingliang</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>


    </section>

  </body>
</html>
