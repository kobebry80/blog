---
layout: default
title:  "分布式FS元数据分析"
date:   2016-05-16
categories: filesystem
---

## 文件系统元数据分类
一般的文件系统会有三类元数据：

- 系统元数据：超级块，集群信息等
- FS元数据：1）名字空间元数据，即目录树；2）文件基础元数据：inode相关的元数据
- 空间映射元数据：文件到块的映射，或者文件到对象的映射

## panFS
总体架构图：

![]({{ site.baseurl }}/assets/file-meta-1.png)

- OSDFS（单盘布局）负责管理空间物理空间映射
- 从一组OSD（一个file的多个分条）中，选择2个以副本的方式存放文件基础元数据(存在对象的属性中)
- 目录也是一个文件，不过它存放的是该目录下的文件列表，以三元组的方式存放<serviceID, PartitionID, ObjectID>; serviceID代表服务它的MDS，partitionID就是我们的逻辑poolid，objectIO就是我们的key。目录文件以RAID1的方式存放，提高效率。
- 对于目录，客户端能够read，cache以及解析。或者client可以直接发送lookup RPC请求到MDS，通过文件名找到对应的<serviceID, PartitionID, ObjectID>以及OSDIDs。然后找对应的MDS提供元数据。当存放元数据的2个OSD都不可用的时候，MDS需要广播getattr命令到所有的OSD去获取元数据。

### FS元数据
基本的元数据操作流程图(创建一个文件)：

![]({{ site.baseurl }}/assets/file-meta-2.png)

- MDS有个操作日志（oplog）；创建文件时，记录创建对象和更新父目录。这个日志能够回滚和前行，如果没有给ack，直接回滚；如果已经给了ack，则需要前进。
- MDS还有个写权限（写锁）日志（caplog），创建文件时候可以返回写锁，或者主动申请写锁。写完成之后，释放写锁，此时删除caplog中的写锁。写锁可以缓存在client，避免每次IO都来申请。
- 日志实现在一个高效的保电内存中（3us延迟）；这个日志还可以配置成主备模式(GE网)，此时延迟大约90us。软件故障主要从本地log恢复，硬件故障需要从备中恢复。
- 如果日志不配置能主备，在一个log损坏时，可能会导致数据不一致。不过这些不一致很容易修复，比如正在读写的过程中发现了就顺路修复；也可以把一些疑似的文件隔离，抽空做一次离线的恢复流程。
- 整个创建操作流程如下:
 1. 将创建请求发给MDS
 2. 在日志记录创建对象（2个元数据对象,RAID1）和更新父目录的信息（操作的所有信息）
 3. 创建文件的元数据对象
 4. 加写锁，并记录到日志
 5. 把对父目录的修改放到reply log，方便batch
 6. 对client返回成功
 7. 把batch到的多个父目录修改一起持久化下去
 8. 删除oplog
 9. caplog在释放锁的时候删除

### 系统元数据
- 一种方案是直接存在后端的对象中去，问题是在存储pool出问题的时候，整个系统不可管理。
- 另外一种方案是多个系统管理进程存放一个副本到本地的数据库，berkleyDB。多个manager之间使用Paxos协议同步。

PanFS选择了第二种，管理信息包括：

- 静态信息：集群中的各个节点信息；用户账号密码等。
- 动态信息：每个服务的状态；


### 空间管理元数据
直接卸载到OSDFS（local盘管理软件），应该就是改造过的本地的FS，与很多主流系统类似。

## GlusterFS
Glusterfs的特点：

- 消除元数据
- 有效的分布数据确保扩展性和可靠性
- 最大化性能

![]({{ site.baseurl }}/assets/file-meta-3.png)

### 元数据管理
网上有篇文章讲解了[glusterfs的设计思路](http://moo.nac.uci.edu/~hjm/fs/An_Introduction_To_Gluster_ArchitectureV7_110708.pdf).

一般分为集中式元数据管理和分布式元数据管理：

集中式如图：

![]({{ site.baseurl }}/assets/file-meta-4.png)

分布式元数据管理的一种方案（每个节点存放全局元数据）如图：

![]({{ site.baseurl }}/assets/file-meta-5.png)

Glusterfs的方案：

![]({{ site.baseurl }}/assets/file-meta-6.png)

- 没有集中的元数据服务，也没有独立分布式的元数据服务器。即元数据管理并不是独立的。
- 集群中的每个server都能不经过第三方查找，而定位到所需的元数据信息；而是通过算法（Elastic Hashing Algorithm）
- 通过路径名＋文件名＋节点视图即可定位到文件的具体位置。

通过算法的好处：

- 直接算出来元数据的位置，效率高
- 某一条元数据只在一个地方管理，没有多点同步开销
- 不会存在旧的元数据版本的问题（同一个元数据放在多个地方可能就会有这个问题）

#### 分布式元数据管理的问题
- 性能开销：分布式锁，分布式事务之类的问题
- 可靠性问题：元数据down掉之后

#### 关于元数据管理的一些讨论
- 如何在线扩容之类，其实完全类似dsware扩容的玩法
- rename，hash类的算法如果rename之后hash值会改变，为了不搬移元数据，对于rename操作直接用新的name link到旧的名字，多了一跳元数据定位。必要的时候可以进行后台搬移。
- 高可用：使用副本的方式，对于每个写操作可以写到多个server上，包括元数据的写。元数据和数据的副本方式应该是一样的。
- 没有提到如何保证一致性和事务性的。个人猜想利用复制技术保证一致性，而每个节点内部的事务性则有FS的本地journal搞定；跨节点的事务如何搞还不清楚。

#### 业界一些分析

[刘爱贵的文章](http://os.51cto.com/art/201403/431809.htm)有一些不错的分析。

[刘爱贵的share材料](http://www.chinacloud.cn/upload/2015-01/15011009251270.pdf)

GlusterFS的哈希分布是以目录为基本单位的，文件的父目录利用扩展属性记录了子卷映射信息，子文件在父目录所属存储服务器中进行分布。

`父目录会在所有的节点都有么？如果不是，那和子文件就不一定在一个节点？似乎没有提到分布式事务的概念`

由于文件目录事先保存了分布信息，因此新增节点不会影响现有文件存储分布，它将从此后的新创建目录开始参与存储分布调度。这种设计，新增节点不需要移动任何文件，但是负载均衡没有平滑处理，老节点负载较重。GlusterFS实现了容量负载均衡功能，可以对已经存在的目录文件进行Rebalance，使得早先创建的老目录可以在新增存储节点上分布，并可对现有文件数据进行迁移实现容量负载均衡。

`父目录和子目录不在一个节点，如何搞定分布式事务？`

[网上关于各个流程的分析：](http://my.oschina.net/uvwxyz/blog/182224)

创建目录的主要步骤有(中间似乎没有事务的概念)：

1. 根据目录名计算哈希值，由其哈希值所在的hash区间确定hashed卷。
1. 向hashed卷下发mkdir操作。
1. 待hashed卷返回后，再向除hashed卷之外的所有子卷下发mkdir操作。
1. 待所有子卷均返回后，合并目录属性。
1. 为每个子卷在该目录上分配hash区间。
1. 将各自的hash区间写入子卷上该目录的扩展属性中。
1. 创建目录结束。
