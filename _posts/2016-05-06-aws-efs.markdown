---
layout: default
title:  "AWS EFS分析"
date:   2016-05-06
categories: filesystem
---


要设计好任何一个软件，用户视图无疑都是最为关键的，即从用户角度来看是如何使用这个系统的。这里简单总结下最近分析的[EFS](http://docs.aws.amazon.com/efs/latest/ug/whatisefs.html)的思路。

### 几个基本概念

对于一个公有云来说，会有很多我们之前没有接触到的概念，包括Region，AZ，Cluster，VPC，sub-Net。

#### Region（区域）：   
全球所有地域的数据中心组成完整的云服务（除了部分地区政策原因无法与其他region交互之外，region之间是需要交互的），每个地域会有多个大型的数据中心。一个地域多个大型数据中心的集合称为一个Region。

- AWS: 目前全球12个Region，还有5个在建
    * 北美4个，南美1个，欧洲2个，亚太5个（包括北京），如下图（括号中为AZ数）
    * 其中，中国的Region与其他地区无法互通（政策原因），即中国的数据无法同步到外部。
    ![AWS](https://d0.awsstatic.com/global-infrastructure/maps/Global-Infrastructure_1.13.png)

- Azure: 目前全球19个Region（5个在建），不过他暂时没有明确AZ的概念。
    * 北美5个，南美1个，欧洲2个，亚太9个（包括北京和上海，由世纪互联运营）
    * Azure也在提Zone Redundant Storage 这个概念，主要是指数据在一个region的不同Zone间复制，不知具体情况如何。

- 阿里云：目前已经建立8个Region（新加坡，香港，深圳，北京，上海，杭州，青岛，美国硅谷）。
![阿里云](http://photocdn.sohu.com/20150515/mp15168097_1431672410560_1_th.jpeg)

#### AZ（可用区）
AZ有独立的供电、独立的网络等，这样假如一个可用区出现问题时也不会影响另外的可用区。在一个区域内，可用区与可用区之间是通过高速网络连接，从而保证有很低的延时。

AZ应该是指同城多个DC的概念，这多个DC用高速网络互连，大约在几KM~几十KM。

- AZ的概念主要来自AWS，Azure似乎不明确（Azure一个region有多个DC，距离在25KM左右，类似AZ）
- 目前AWS一个region的AZ数目主要为2, 3, 5.
- [阿里云也有可用区的概念](http://www.cnblogs.com/cmt/p/aliyun-region-availability-zone.html)，AZ数量主要为1-3个（创建VM时，需要手动选择Region和AZ）

#### Cluster（集群）
集群的概念咱们接触的相对比较多，主要指一个AZ内部多个机架组成的一个逻辑集群，集群内部节点网络对称互连（任意两个点都是一样）。

一个集群所有的节点通过分布式软件统一池化，每个节点都能访问其他所有节点。

FusionStorage目前规格是一个集群4096个OSD节点，10240个Client节点。（每个节点12盘，单盘4TB，接近200PB容量）

AWS还没有找到对应的资料。

Azure表示一个集群能支撑20个Rack，每个Rack 18个存储节点，总计360个节点，最大支持30PB容量（来自论文Azure Storage）。

#### VPC
AWS比较早推出[VPC](https://aws.amazon.com/cn/vpc/)这个概念。

VPC是用来架设独立于互联网的子网的，就相当于自己架设一个独立的局域网。然后想连到互联网的话需要自己配置防火墙，路由，网关等。

对于文件服务这种云内部使用的服务，网络隔离非常重要。

VPC一般可以跨多个AZ存在，如下图。

![](http://docs.aws.amazon.com/efs/latest/ug/images/overview-flow.png)

在一个VPC内部，网络是互通的，即同一个VPC的EC2实例可以挂载这个VPC内的任何FS（当然需要FS在VPC内提供mount target，相关于NAS server）。一般一个租户会为自己建立一个独立的VPC，实现网络的隔离。


#### [Sub-Net](http://docs.aws.amazon.com/zh_cn/AmazonVPC/latest/UserGuide/VPC_Subnets.html)

连接中AWS对于VPC和Sub-Net都有定义，每个子网都必须完全位于一个可用区域之内，不能跨越多个可用区域。

一般子网会有公有子网（能与外部internet通信）、私有子网（无法与公网通信），仅限VPN子网（数据流会被路由到虚拟专用网关）。

对于文件服务来说，mount target一般会用私有子网，因为文件服务暂时只支持在云数据中心内部使用（后续可以考虑任意地方访问）。

VPC和Sub-Net主要是网络的概念，存储后端的网络可能会继续独立于计算网络，但是存储的前端必然会与VPC和Sub-Net配合，所以还是需要了解的。

### 如何使用Cloud FS
这里主要内容参考EFS的User guild有兴趣的同学建议阅读原文。

#### 环境准备
1. 注册一个公有云的账号，比如：ming
2. 登录个人的管理界面
3. 创建一个VM实例并启动（AWS成为EC2），这个VM实例创建时候需要指定Region，AZ，VPC等。
  - 选择镜像
  - 选择规格（CPU，Memory等）
  - 配置网络，选择默认的VPC，选择子网（可以选择任意一个AZ中的一个子网），也可以新建一个子网。
  - 配置安全组
  - 启动VM实例

#### 准备文件系统
##### 创建文件系统
1. 登录文件系统管理界面或者直接使用cli
2. 创建文件系统，选择与前面创建VM相同的VPC
3. 选择FS在每个AZ的mount target（如何创建，见后文）
4. 创建成功后会获取到FS ID
5. 为FS打个tag（取个name），方便对外展示&辨识。

```
AWS创建FS的命令为：
$ aws efs create-file-system \
--creation-token creation token \
--region aws-region \
--profile adminuser
```
##### 创建mount target
VM实例要访问FS，必须通过一个mount target，它负责I/O转发，NAS协议解析，安全策略控制等。

创建mount target步骤如下：

1. 登录管理界面，或者直接使用cli
2. 选择要创建mount target的AZ（建立每个AZ都创建一个，避免VM跨AZ去访问FS）
3. 在每个AZ内部：
 - 选择创建mount target的子网
 - 让mount target的IP自动配置（AWS是能够自动配置的）
 - 选择安全组
4. 启动所有的mount target
5. 查看mount target的DNS name，挂载时候需要用这个（通过DNS服务将DNS name转换成mount target的IP）。

```
DNS name:
availability-zone.file-system-id.efs.aws-region.amazonaws.com 
```

```
AWS创建mount target的命令为：
$ aws efs create-mount-target \
--file-system-id file-system-id \
--subnet-id  subnet-id \
--security-group ID-of-the-security-group-created-for-mount-target \
--region aws-region \
--profile adminuser
```

#### 使用文件系统
1. 挂载FS
  - 用DNS name挂载：用DNS更好，可以永远不变（IP是有可能会变化的）
  - 用IP挂载：比较常规的方式，查询VM对应的mount target的IP，然后手动挂载
  - 自动挂载：在 /etc/fstab添加对应的FS，VM启动后自动挂载
2. 使用文件系统
  - 对用户进行[权限设置](http://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html)，创建FS的root user有所有权限，它能设置其他访问者的权限。
  - 统计信息：用户能否看到[FS size以及FS的对象size](http://docs.aws.amazon.com/efs/latest/ug/metered-sizes.html)，方便计费。EFS元数据2KB，数据块是4KB的整数倍
  - 像一个本地目录一样去操作FS：但是要明确一些[不支持的操作](http://docs.aws.amazon.com/efs/latest/ug/nfs4-unsupported-features.html)，比如EFS就不支持强制锁，有访问冲突时需要用户自己去加锁。
  - 还有一些[规格约束](http://docs.aws.amazon.com/efs/latest/ug/limits.html)
3. [使用案例](http://docs.aws.amazon.com/efs/latest/ug/wt2-apache-web-server.html)
4. 备份FS：EFS能够将文件系统备份到另外一个EFS文件系统。使用AWS的Data pipeline技术。并不是文件系统到S3的备份模式（这块可以考虑与混合云多交流下）。
5. EFS的一些[管理API](http://docs.aws.amazon.com/efs/latest/ug/api-reference.html)

#### 清理文件系统
任何时候都要记得操作的对称性：
1. 卸载文件系统
2. 删除挂载点
3. 删除文件系统：永久删除和回收站
4. 删除VM实例

## 再次测试EFS

上次测试只是从2个AZ进行了测试，这次直接三个AZ全部测试下，重点关注延迟。IOPS由于QoS控制测不出来效果。

## AZ内部测试

三个VM分别写本AZ的target(测试三次)：

AZ1:VM1平均延迟：9.44ms，9.49ms，9.52ms
AZ2:VM2平均延迟：8.92ms，8.94ms，8.92ms
AZ3:VM3平均延迟：10.17ms，9.99ms，10.02ms

似乎很明显的阶梯状，AZ2是主，但是为啥AZ1和AZ3差距这么大？


三个VM分别写本AZ的target(测试读三次)：

AZ1:VM1平均延迟：2.52ms，2.63ms，2.62ms
AZ2:VM2平均延迟：2.75ms，2.66ms，2.66ms
AZ3:VM3平均延迟：2.87ms，2.78ms，2.77ms

单队列读延迟来看，规律不明显，但是VM3似乎要差一点点。100us的样子，我们再看看读的峰值带宽。

AZ1:VM1:104MB/s
AZ2:VM2:105MB/s
AZ3:VM3:102MB/s

似乎没有差别，继续看看读IOPS(32队列，4KB随机读)：

AZ1:VM1:6350,7061,7199
AZ2:VM2:7452,7405,6482
AZ3:VM3:7463,7451,7317

```
sudo fio -filename=/mnt/efs1/test -direct=1 -iodepth 32 -thread -rw=randread -ioengine=libaio -bs=4k -numjobs=1 -runtime=30 -group_reporting -name=mytest
```

## 跨AZ测试

VM1测试各个AZ的性能，写延迟：
AZ1:9.37ms,9.22ms,9.34ms
AZ2:9.56ms,9.59ms,9.52ms
AZ3:10.87ms,10.59ms,10.43ms

与前面数据对比大致可以看出来跨一次AZ延迟大约500us。基本可以确信AZ2就是主。

VM2测试各个AZ的性能，写延迟：
AZ2:8.91ms,9.04ms,8.92ms
AZ1:9.77ms,9.83ms,9.69ms
AZ3:11.38ms,11.1ms,11.04ms

AZ2访问AZ3的性能非常诡异，理论上应该跟AZ1访问AZ3类似啊？

VM3测试各个AZ的性能，写延迟：
AZ3:9.98ms,10.48ms,9.99ms
AZ1:9.24ms,9.35ms,9.40ms
AZ2:9.39ms,9.67ms,9.50ms

VM37-28日测试，写延迟
AZ3:9.75ms,9.47ms,9.44ms
AZ1:9.13ms,9.21ms,9.14ms
AZ2:9.63ms,9.64ms,9.66ms

最后这一组数据非常奇怪，访问本地反而比跨AZ慢，而且访问主反而比访问非主慢？是否AZ3到AZ1的网络比较好？

根据以上数据，基本可以分段推断出一组数据(明天画个延迟分段图)

